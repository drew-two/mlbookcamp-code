# 10.9 - Summary

![Overview](../images/10.1-overview.png)
- We use TF-serving to serve our model
    - Optimized just for inference. Cannot train or anything
    - Uses gRPC for communication - optimized binary format
- Needed 'gateway' to preprocess image and use gRPC to communicate with TF-serving
- System is more complicated but has some benefits
    - Can split gateway/website and inference part - can scale independently
        - Gateway/website can just use CPU
        - Inference service can use GPUs

We ran the gateway and TF-serving locally at first
- Learned **docker-compose**: convenient way of running multiple services and linking them locally
- Installed `kubernetes` and `kind`
    - `kind` is a lightweight kubernetes server you can run locally
    - Uses docker to setup K8s cluster
    - Deployed our application to K8s

Deployed application to EKS
- Installed eksctl to control EKS from command line
- NOT free

## Learn more

* Other local K8s instances: minikube, k3s, k3s, microk8s, EKS Anywhere
    - EKS Anywhere is like EKS locally
    - minikube uses VirtualBox as backend
* Desktop support
    * [Rancher desktop](https://rancherdeskto.io/)
        - Like Docker desktop but for Kubernetes
    * Docker desktop has Kubernetes support (Windows)
    * [Lens](https://k8slens.dev/)
* Many cloud providers support Kubernetes: GCP, Azure, Digital Ocean, etc
    - Look for "Managed Kubernetes" in web search
    - Config files should work anywhere (besides image URIs)
* Deploy the model from previous modules and from your project with Kubernetes
    - E.g load the tf-lite model with Kubernetes
* Learn about Kubernetes namespaces. Here we used the default namespaces
    - Allows you to logically separate something and have the same naming locally
        - E.g. Service, whole project, company team etc