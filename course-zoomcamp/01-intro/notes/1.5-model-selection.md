# Modelling Step of CRISP-DM: Model Selection

Assume data is prepared to be used for ML immediately.

Which model to choose?
- Logistic regression
- Decision tree
- Neural network
- Etc.

Need a consistent way to select a model.

## Selecting the Best Model
- Say it is July, and we have *X* and *y*, and we have produced model *g*.
    - We apply *g* to emails in August to see if they are spam. Prediction: 0.7
    - In July we cannot evaluate the model, but in August on new data we can.
    - We want to mimick this process for evaluation

## Validation Data
- We can take a small part of data, say 20%, and pretend it does not exist.
    - We use the remaining 80% for training.
    - We call the held-out data **validation data**
- We have a train split and a validation split.
    - Training data: extract *X* and *y*. We train *g* on this.
    - Val data: extract a new *X<sub>v</sub>* and *y<sub>v</sub>*
        - Take *g*, apply on *X<sub>v</sub>*. *g*(*X<sub>v</sub>*) = *ŷ<sub>v</sub>*

## Validating Model
- Compare *ŷ<sub>v</sub>* with actual values *y<sub>v</sub>*
    - E.g. Take predicted probabilities, apply >= 0.5 rule
        *ŷ<sub>v</sub>*     *y<sub>v</sub>*
        0.8 - **1**              1
        0.7 - **1**              0
        0.6 - **0**              1
        0.1 - **1**              0
        0.9 - **1**              1
        0.6 - **1**              0
            pred                target
    - 4 cases correct out of 6; 66% correct.
- Apply this validation for each model
    - *g<sub>1</sub>* Logistic Regression   - 66%
    - *g<sub>2</sub>* Decision Tree         - 60%
    - *g<sub>3</sub>* Random Forest         - 67%
    - *g<sub>4</sub>* Neural Network        - 80%
- We see the NN has the best accuracy and select this model

## Multiple Comparisons Problem
- Say you have a 20% validation set of emails, and 4 coins.
    - For each coin, you flip it and record heads or tails for each email.
        - Heads is spam, tails is not spam
    - Say your first coin gives 20% accuracy
        - Your second coin gives 40% accuracy
        - Your third coin gives 60% accuracy
        - Your fourth coin gives 100% accuracy
- Normally, you would end up choosing the fourth coin. 
    - However, by inspection we can see that this is just luck, the coin really only has a 50% chance of choosing the correct answer.
- Say each coin is an ML model you tested for selection
    - This means the NN is the best. But we know that this could be a fluke due to luck
    - The NN only *happens* to be the best on this subset of data. It could fail on any other 20% subset of the training data.
    - These are probabilistic models, so they can just get lucky.
- How do we solve this?

## Validation and Test Datasets
- To guard against this issue, we do not just hold out one dataset, but two datasets.
    - Take 20% for validation purpose, and another 20% for testing purposes
    - The remaining 60% is used for training.
    - Note that the 60-20-20 split is not set in stone, you can use other ratios
- Now, we hide the test dataset and do not look at it.
- Take  *X*, *y*, *X<sub>v</sub>*, and *ŷ<sub>v</sub>*
    - Train *g* like normal, and get accuracy
    - Select best model. E.g. NN
        - To make sure the model did not just get particularly lucky on the validation dataset, do another round of validation on the test dataset (*X<sub>t</sub>*, and *ŷ<sub>t</sub>*)

## Testing Model
- Get models after testing on validation dataset
    - *g<sub>1</sub>* Logistic Regression   - 66%
    - *g<sub>2</sub>* Decision Tree         - 60%
    - *g<sub>3</sub>* Random Forest         - 67%
    - *g<sub>4</sub>* Neural Network        - 80%
- Take best model, just NN, and apply test dataset
    - It may get a good result still, and we decide the model is good enough

## 6 Steps of Model Selection
1. Split
    - Split Dataset into train/validation/test
2. Train
    - Train the model on only train dataset
3. Validation
    - Apply model to validation dataset. Get validation accuracy
    - Repeat steps 2 and 3 as many times as you need for each model
4. Select the best model
5. Test
    - Apply model to test dataset. Get test accuracy
6. Check
    - Make sure everything is good

Generally, between 4 and 5, we can see that the data used for validation is kind of wasted.
- So, what we can do, is merge the validation set and training dataset back together.
    - Retrain on this new train dataset, and move on to step 5 for testing
        - This should be better as there is more data than before
- This way we are not wasting the validation dataset. Can do the same thing with test data when deploying.

# Summary
- Choose candidate models
- Split dataset into training, validation and test datasets with a 60%/20%/20% split.
    - Not set in stone, can adjust ratio as need be
- Train models and use validation data to assess performance
- Take best model(s) from validation
    - Can retrain and training + validation data combined here
- Take best model(s) and apply test dataset
- Check, see if test accuracy is good. If so, take model to next step.