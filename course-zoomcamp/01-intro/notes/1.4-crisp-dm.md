# CRISP-DM

Taking a step back from types of ML to understanding ML methodology.

## ML Projects
ML projects have 4 basic steps:
1. Understand Problem
2. Collect the data
3. Train the model
4. Use it

## CRISP-DM
Set of CRoss InduStry Standard Process for Data Mining
- Methodology for organizing and planning machine projects
    - Fairly old, invented in 1990s by IBM
    - Still useful with almost no modification, though some elements are obselete
- Presented as a cycle:
    1. Business understanding
    2. Data understanding (may go back to step 1)
    3. Data Preparation
    4. Modeling (may go back to Data Preparation)
    5. Evaluation (may go back to Business understanding)
    6. Deployment
    - Repeat

## Spam detection example
1. Business Understanding
    - Want to understand if problem is important
    - Need a way to measure success of solution
    - Do we need ML at all? When you have a hammer, everything looks like a nail
    - Problem: Our users complain about spam
        - Analyze to what extent it is a problem
            - Are there a lot of people affected by this?
            - Can we find a success metric?
                - Can use these two to find if we should spend time on this
        - Will Machine Learning help? 
            - If not: propose and alternate solution
                - Maybe we could use a rule-based or heuristic system
                - Do not need the time and resources for an ML system
    - Define the goal:
        - Reduce the amount of spam messages, OR
        - Reduce the amount of complaints about spam
    - Goal has to be measurable. Now that we can measure the amount, can metric based on that
        - Reduce the amount of spam by 50%
2. Data Understanding
    - Analyze avaible data source, and decide if we need to get more data
        - May need to buy data
        - May need to collect more data
    - Identify data sources
        - We have a spam report button
        - Is the data behind this button good enough?
            - What happens when a user clicks it?
        - Is it reliable?
            - Is it recorded for every click?
            - Maybe users say things are spam when they are not
                - ML model may end up misclassifying spam
        - Do we track it correctly?
        - Is the dataset large enough?
            - Do we need to get more data?
                - Can be the output of this step
                    - E.g. We need 2000-3000 records before we can start
    - Analyze problem, identify data sources, see what is missing, see if there are problems in tracking
    - This step may influence the goal
        - We may go back to Business understanding and adjust it
3. Data Preparation
    - Transform the data so it can be used for an ML algorithm
        - Usually extracting different features from data
    - Data Preparation
        - Clean the data
            - Remove noise (e.g. users accidentally marking spam)
        - Build the pipelines
            - Code sequence of steps, applies some transformation(s), produces clean data
        - Convert to tabular data
    - Example
        - Take all data (emails and label), feed to data processing pipeline
            - Output as table
                - Fields may be sender, receiver, subject, etc.
                - Target will be spam; TRUE or FALSE
            - Extract features from table
                - Recall feature extract to vector of 0's and 1's
    - Output data formatted as *X* and *y*
4. Modeling
    - Training a model (actual ML happens here)
        - Try different models
        - Select the best one
    - Which model to choose?
        - Logistic Regression
        - Decision tree
        - Neural network
        - Etc.
    - Sometimes, we may go back to data preparation:
        - Discover feature extracted are not sufficient; add new features
        - Discover data issues; go back to fix them
5. Evaluation
    - Measure how well the model solves the business problem
    - Is the model good enough?
        - Have we reached the goal?
        - Do our metrics improve?
    - Goal: Reduce the amount of spam by 50%
        - Have we reduced it? By how much?
            - E.g. reduced by 30%. Is this good enough?
    - Perform retrospective:
        - Was the goal achievable?
        - Did we solve/measure the right thing?
    - After that, we may decide to:
        - Go back and adjust the goal
        - Roll the model to more users/all users
        - Stop working on the project
6. Evaluation + Deployment
    - Often happens together. 
        - CRISP-DM is old, we often deploy on some users, and collect data there.
            - Called **online evaluation**
            - It means: deploy the model, and evaluate it on real users
7. Deployment
    - Roll the model to all users
    - Proper monitoring
    - Ensuring the quality and maintainability
    - Best engineering practices really come in here
        - Reliability, scalability
8. Iterate!
    - ML projects require many iterations
    - After deployment
        - Can we improve it?
        - Should we improve it?
        - Maybe we leave it, and come back a year later, and repeat process
            - Find new business goal, potentially have new data or can get new data
    - Always start simple
        - Do something simple and quick each time
        - Learn from feedback
        - Improve
        - Allows quick, small iterations and quick feedback

# Summary
- Business understanding: define a measurable goal. Do we actually need ML?
- Data understanding: do we have the data? Is it good?
- Data preparation: transform data into table to be used for ML.
- Modelling: to select the best model, use the validation set.
- Evaluation: validate that the goal is reached
- Deployment: roll out to production to all the users
- Iterate: start simple, learn from feedback, make multiple iterations.