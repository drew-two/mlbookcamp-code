# Supervised Machine Learning

## Recall
In 1.1 we used a car price prediction example
- In a sense, we "supervised" the model, by showing a price for each car
    - We did this for all the cars we had

In 1.2 we used a spam prediction model
- We knew if each email was spam or not
- We call this **supervised** ML

## Supervised ML
**Supervised** ML: Models are trained on examples that have ground truth labels
- Usually, we need to extract features
    - Car information (make, model, etc.)
    - Email metadata (sender, sender domain, title length, etc.)
- What we want to predict (car price, spam)
    - Model ultimately only knows if answer is 0 or 1
- Machine Learning is a subset of Statistics and Applied Mathematics
    - Apply statistical techniques to features to extract patterns
    ```
                    Features			Target
                    (data)          (desired output)
                [1, 1, 0, 0, 1, 1]			1
                [0, 0, 0, 1, 0, 1]			0
                [1, 1, 1, 0, 1, 0]			1
                [1, 0, 0, 0, 0, 1]			1
                [0, 0, 0, 1, 1, 0]			0
                [1, 0, 1, 0, 1, 1]			0
    ```
    - These are our **Feature matrix**, *X*, and **Target vector**, *y*
        - Can think of *X* as a 2D array, and *y* as a 1D array
- Now we have to feed *X* and *y* to a model.
- When we train a model, we want our predictions to be as close to *y* as possible.
    - Simply, this is 
        *g*(*X*) ≈ *y*
        - Where *g* is our model, *X* is our features and *y* is our target
    - Want to build a model *g*, that returns predictions *g*(*X*) as close to *y* as possible
        - Not possible to predict exact class, but want to be as close as possible

## Email Example
```
    Features			Predictions     Target
    (data)              (output)    (desired output)
[1, 1, 0, 0, 1, 1]			1               1
[0, 0, 0, 1, 0, 1]			0               0
[1, 1, 1, 0, 1, 0]			1               0
[1, 0, 0, 0, 0, 1]			1               1
[0, 0, 0, 1, 1, 0]			0               0
[1, 0, 1, 0, 1, 1]			0               1
```
- Model takes *X* and produces *g*(*X*)
    - Want *g*(*X*) to be as close to Target *y* as possible

## Regression
- Car price prediction is *Regression* - predicting a continuous value from $0 - $∞
- There are other Regression examples, any continuous value in range
    - E.g. House price prediction -> predict house is $1M

## Classification
- Classifying what is in a given example
    - E.g. looking at a picture of car and outputting "Car"
    - E.g. given an email, output spam/not spam
- Notably, *g*(*X*) is *always* a discrete variable
- Multi-class Classification
    - Choosing from more than 2 (binary) classes
    - E.g. Given image, identify cat/dog/car
- Binary Classification
    - Choosing from 2 classes
    - E.g. Given email, identify spam/not spam

## Ranking
- Will not be used extensively in course
- Where you want to rank something, e.g. Recommender system
    - E.g. On e-commerce website, what should we present to a user?
        - Given an item, what is the probability (e.g. 0-1) the user would like it
        - Order the items based on probability and return items in that order
        - Show top-x results
    - E.g. Google returning results
        - Web pages are ranked by relevance score and returned in that order
    - E.g. E-commerce platform searching
        - Possible products are ranked and returned

# Summary
- Supervised learning takes a labelled matrix of examples and features matrix *X*, with labels *y*
    - These are fed it to model *g*, which returns *g*(*X*) such that it is as close to *y* as possible.
    - *g* tries to extract patterns from *X* to facilitate this
- Predictions can be *Regression*, *Classification* or *Ranking*
    - Classification can be
        - Binary
        - Multi-class
- Will focus mostly on classification in this course
- Binary classification is the most widely used type of supervised ML