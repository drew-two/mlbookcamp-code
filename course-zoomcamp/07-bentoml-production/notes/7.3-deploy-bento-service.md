# 7.3 - Deploying Your Prediction Service

Recap: Last module we built the prediction service by:
1. saving model
2. packaging model
3. deploying it as bento

- Let's take a closer look at what BentoML has been doing behind the scenes
    - Then, we will build bento and see: all the dependencies, metadata and objects that get packaged in automatically
    - Then, build docker container and run it

- Run `bentoml models list`
    - Can see model tags on the left, then framework (xgboost, sklearn etc), model size, and creation time 
    - What we are saving automatically, we can see with `bentoml models get <name>:<tag>`
        - Can see framework, framework version, and python version especially
            ```
            name: mlzoomcamp_homework                                                       
            version: qtzdz3slg6mwwdu5                                                       
            module: bentoml.sklearn                                                         
            labels: {}                                                                      
            options: {}                                                                     
            metadata: {}                                                                    
            context:                                                                        
            framework_name: sklearn                                                       
            framework_versions:                                                           
                scikit-learn: 1.1.1                                                         
            bentoml_version: 1.0.7                                                        
            python_version: 3.9.12                                                        
            signatures:                                                                     
            predict:                                                                      
                batchable: false                                                            
            api_version: v1                                                                 
            creation_time: '2022-10-13T20:42:14.411084+00:00'
            ```
        - **Very important** to get exact same framework versions
            - Other stuff is nice to have if not necessary
- In module 5 we looked at finding module versions and building with Pip env
    - BentoML will automatically find versions and build Docker with this

- Want to build single-unit deployable with all this information
    - Need **bentofile.yaml**
        ```
        service: "service.py:svc"
        labels:
            owner: bentoml-team
            project: gallery
        include:
        - "*.py"
        python:
            packages:
                - xgboost
                - sklearn
        ```
        - Specifies entrypoint (service)
        - Has labels (any key value pair) to make any tags you need
        - `include/exclude` parameters
            - Only pull in files you want and no others
            - E.g. in larger repository with other Python files you don't need
        - Can specify python packages bringing in
- Go to [documentation](https://docs.bentoml.org/en/latest/concepts/bento.html#files-to-include)
    - Can see what we can include, exclude, how to specify Python packages/versions
        - Can specify Pip env versions, wheels, Conda options,
        - Can specify Docker options, distro/distro version, the python version, cuda version, system packages, env variables
            - Will warn if versions are not all compatible
- Building is simple, can do with `bentoml build`

- Can see tag after run finishes.
- Can see what was saved under `~/bentoml/bentos/credit_risk_classifier:latest`
    - Run `tree`
    - 
    ```
    /home/andre/bentoml/bentos/credit_risk_classifier/2bx5umsuccwgeaav/
    ├── README.md
    ├── apis
    │   └── openapi.yaml
    ├── bento.yaml
    ├── env
    │   ├── docker
    │   │   ├── Dockerfile
    │   │   └── entrypoint.sh
    │   └── python
    │       ├── install.sh
    │       ├── requirements.lock.txt
    │       ├── requirements.txt
    │       └── version.txt
    ├── models
    │   └── credit_risk_model
    │       ├── latest
    │       └── vrwnb4subwmjaaav
    │           ├── custom_objects.pkl
    │           ├── model.yaml
    │           └── saved_model.ubj
    └── src
        ├── code
        │   ├── locustfile.py
        │   └── service.py
        └── service.py

    9 directories, 16 files
    ```
    - Makes us:
        - OpenAPI spec
        - Dockerfile
        - Python requirements
        - Model itself with the versions, custom objects (dv), metadata (yaml) and saved model (ubj)
        - Source code - only one we really need is src/service.py
- Made to contain everything you need for a service 
    - When you containerize you have a single image you can deploy into a lot of different environments

- Containerize with `bentoml containerize <service name>:<tag>`
    - May take a while the first time
- Can see image with `docker images`
    - Run latest with `docker run -it --rm -p 3000:3000 <service name>:<tag>`
- With container running, go back to `http://localhost:3000/` to test.
