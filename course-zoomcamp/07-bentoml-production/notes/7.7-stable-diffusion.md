# 7.7 - (Optional) Advanced Example: Deploying Stable Diffusion Model

Based on new StabilityAI model: Stable Diffusion

# Pre-built Bento
Instructions to build with BentoML on [GitHub](https://github.com/bentoml/stable-diffusion-bentoml)
- Follow instructions under [Prepare the Environment](https://github.com/bentoml/stable-diffusion-bentoml#prepare-the-environment)
    - Installs requirements in a local virtual environment
    - Mostly BentoML, pytorch and HuggingFace transformers

# Looking at model
Instructions under [Create the Stable Diffusion Bento](https://github.com/bentoml/stable-diffusion-bentoml#create-the-stable-diffusion-bento). We will build from the model
- Go to [Build from Stable Diffusion Models](https://github.com/bentoml/stable-diffusion-bentoml#build-from-stable-diffusion-models)
    - Go to fp16/ `cd fp16/`
    - Download the model for fp16
        - **Takes awhile to download**
    - Go to directory models/. Type `tree` to see all the data for the model
    - Look at bento: `vi service.py`
        - Functions at the bottom are familiar
            - Can see the runner for Stable Diffusion
                - As a different process as its own custom runner to use different CPUs
            - Text to image API takes in JSON and returns image
                - JSON is field with some text, then that text is passed to the runner
            - Image to Image API takes in Multipart spec: JSON and an Image. Then returns image
                - Both image and text passed to runner
        - `StableDiffusionRunnable` class: Custom runner for StableDiffusion
            - Specifies GPU with `SUPPORTED_RESOURCES` to "nvidia.com/gpu". We are pinning this 
            - Specifies CPU multithreading with `SUPPORTS_CPU_MULTITHREADING`
            - `__init__()` takes model id from model/v1_4_fp16 and uses cuda device
            - Basically where you pull in everything you need to run a prediction
                - Trained model, some logic, augmentation, pipelines, etc
            - Specify `@bentoml.Runnable.method()` decorator for the inference functions
                - `txt2img` gets text and returns image. Tries use paramters if given, otherwise has defaults
                - `img2img` takes image and text and returns image.
                    - May do pre-processing and post-processing
                        - These may scale with API workers and top-level processes
                        - Let's you scale separately from inference and batch if you want, and fan out if you want
    - Look at bentofile: `vi bentofile.yaml`
        - Specifies which services, files and fields to include
        - Specifies packages needed
        - Specifies docker options:
            - Debian by default
            - **Needs `cuda_version` for Nvidia GPU**
                - Most in AWS are cuda ver 11.0+, so you would need to specify `11.6.2`
            - Env file like `bento_config`, controls attributes of the runner
                - Here it is `src/configuration.yaml`
    - Look at config file: `src/configuration.yaml`
        - Just specifies timeout

### Building bento
Run `bentoml build` in fp16/ directory
- Can run locally if you want but it needs an Nvidia GPU

## Deploying bento
Installing from [Deploy the Stable Diffusion Bento to EC2](https://github.com/bentoml/stable-diffusion-bentoml#deploy-the-stable-diffusion-bento-to-ec2)
Will need [bentoctl](https://github.com/bentoml/bentoctl)
- Cloud agnostic. Install with `pip install bentoctl`
- Install EC2 support for Terraform with `bentoctl operator install aws-ec2`

Deployment already configured in /bentoctl directory
- See with `vi /bentoctl/deployment_config.yaml` 
    - Straightforward; using AWS-EC2 with Terraform template
    - **Note:** instance used is `g4dn.xlarge`
        - One of the cheaper GPU instances
        - Uses deep learning ami `ami-0a85a3a3fb34b3c7f`: *Deep Learning AMI GPU PyTorch 1.12.0 (Ubuntu 20.04) AMI*
    - Auto deploys to `us-west-1`
- Generate deploy Terraform template from the config: `bentoctl generate -f deployment_config.yaml`
    - Generates main.tf and tfvars file
- Build with `bentoctl build -b stable_diffusion_fp16:latest`
    - *Note:* We need a special build container because EC2 has some different specific requirements
        - Motivation for BentoCTL
    - Anything that doesn't need special settings can use `bentoml containerize`
- Push to ECR and apply to ECR with `bentoctl apply -f deployment_config.yaml`
    - Can test with the IP returned at port 3000
- Delete deployment with `bentoctl destroy -f deployment_config.yaml`

# Summary
- Build with `bentoml build` in the model folder
- Use BentoCTL to create Terraform templates to deploy to EC2
    - `bentoctl build -b stable_diffusion_fp16:latest`
- Apply with `bentoctl apply` or `bentoctl apply -f deployment_config.yaml`
    - Get the IP as well
- Delete with `bentoctl destroy -f deployment_config.yaml`