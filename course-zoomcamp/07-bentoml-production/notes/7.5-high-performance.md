# 7.5 - High-Performance Serving

## Overview:
1. Mock high volumes of traffic
2. Walk through optimizations to be able to handle this traffic

## Traffic Simulator
- Need a traffic simulator to send traffic at service
- Will use **Locust** - `pip install locust`
- Need to create [locustfile](../code/locustfile.py)
    - Contains:
        - Sample of data you will send
        - Create HttpUser class inheriting from locust.HttpUser
            - Create task `classify` under class
            - Just like Swagger UI but running at 10s/100s/1000s requests per second
        - Defines wait_time between each time dummy users hit endpoint
            - We will choose random time in a range - `wait_time = between(0.01, 2)`

## Running Locust
Run locustfile with `locust -H http://localhost:3000` where the URL is the URL you want to hit
- Go to locust UI: http://0.0.0.0:8089
    - Notice the parameters:
        - Number of users (peak concurrency)
        - Spawn rate (users started/second)
        - Host (URL to hit)
    - Recall that the default setting for failure in BentoML is if a request takes longer than 1 second
        - Can be changed to anything or infinite
        - Usually a program needs to have a threshold to fail though
- Start swarming - can see the stats with around 0.5-1 RPS
    - Can see requests in server
- Dashboard shows overall dashboards
    - Median request latency (4ms)
    - 90%ile and 99%ile ms and maximum overall
- Ramp up locust - 50 users & 10 spawn rate
    - RPS goes from 25 to 50
    - Can see latencies going up at the high end (>1000 ms)
    - Failures goes up - 5-10%

## Optimizing deployment
Fairly easy, pattern popularized NodeJS
- Without `async await`, every time a request comes in, Bento has to service a request before going to the next request
    - Thus we must service requests sequentially instead of in parallel
- Async allows for asynchronous execution at the endpoint level - allows parallelization
    - BentoML allows for asynchronous execution at the inference level
        - Using `await` and `async_run` when predicting
            - `prediction = await model_runner.predict.async_run(vector)
- Sometimes there are a few failures at the very beginning because of *cold start issue*
    - This is when a server has not cached anything at startup before reaching peak performance
    - Easily doing ~50 RPS with 4 ms median and 14ms 99%ile
- Let's bring this up to 200 users and spawn rate 50
    - RPS ramping up with similar latency at thhe low end but 150 ms 99%ile
- Bringing it up to 300 users
    - Median latency 5ms, high end almost 300 ms, max at ~1000ms
    - 0 failures approaching 300
- Pushing to 1000 users and 100 concurrent with 0 failures
    - High end ms now close to 1000 ms and approaching 26000 ms at the max

Typically when you start a service on your machine, you're starting one process, which is run by the CPUs on your machine
- The problem is that, when you only have 1 process, only 1 CPU can work on it at a time
    - Want to have multiple processes to utilize more CPUs
- Traditional web scaling replicates the entire process
    - Each process gets 1 web worker and 1 model
    - The issue with this, with larger models you can only pull it into RAM once or twice more without running out of space
- If you have 1 model, it is significantly more efficient to send inputs to 1 model at once rather than 1 at a time
    - E.g. in module 6, when sending a spreadsheet full of predictions to a model
        - These are all evaluated at once - this is significantly faster than running 1 at a time
- Thus, if we can combine these rows we are inputting to the model in one vector we can find huge efficiency boosts
    - We do this by fanning out web workers, and sending their batches to one batch to be predicted by the model
    - This is what the `runner` does in BentoML
- Need to apply this when saving the model:
    - In the `save_model` function, add another parameter `signatures`
    ```    
    signatures={
        "predict": {
            "batchable": True,
            "batch_dim": 0
        }
    }
    ```
    - This sets the `predict` method to be batchable, and the batch dimension is 0
    - Concatenating arrays by the first dimension.
        - If it were by the second dimension it were different but BentoML does this simply by concatening inputs into one array
- Model was saved to `cacobpdflgrusaav`
    - Edit service.py to use this instead
    - Now call `bentoml serve --production`
        - This flag also says we want more than one process for our web workers
- Go back to locust
    - Set 400 users with 50 concurrent
    - Failures spike to ~25% at startup but drop to around ~0-1%
    - Median ms is around 32, 99%ile is 34, max is 114 ms
- BentoML tunes for ideal batch size based on prior batches run
    - Can specify maximum batch size or or don't wait for longer than certain amount of time to get median time down
- Try running 1000/100 on Locust
    - Failures to 1% around 600 RPS but median is 220ms with 770 90%ile and max ~10000ms
- Can see more about adaptive batching in [documentation](https://docs.bentoml.org/en/latest/guides/batching.html)
    - ![Architecture](https://docs.bentoml.org/en/latest/_images/batching-architecture.png)
    - We have a software load balancer that takes all requests and sends to API server 
        - API servers are the processes mentioned previously
        - The runner is where all the batching is happening, and then sent to the model
- Important parameters:
    - `max_batch_size`: Don't send more than a certain number of inputs in one batch
        - Say performance degrades over 100 inputs at once, we can set this to 100 maximum in one batch
    - `max_latency`: E.g. don't wait more than 5ms before sending batch to the model
        - Something you should play with more to get latency down further
- Monitoring to show you batch sizes and latency over time

- Can edit configuration with `bentoconfiguration.yaml`
    ```
    # Config file controls the attributes of the runner
    runners:
        batching:
            enabled: true
            max_batch_size: 100
            max_latency_ms: 500
    ```
    - A lot more resource allocation options in [documentation](https://docs.bentoml.org/en/latest/concepts/runner.html)
    - Can specify amount of cpu to use, whether or not and how many GPUs to use
    - Can specify timeout
- Gets interesting when you have multiple runners
    - If you want to await multiple models before doing something else, keeping these in separate processes when batching is happening is ideal
    - Complex under the hood but very simple abstractions for deployment with simple async interface