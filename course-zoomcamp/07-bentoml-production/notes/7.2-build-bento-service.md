# 7.2 - Building Your Prediction Service with BentoML

- Taking model from module 6 and building Bento to service this
- Look at [Notebook](../../06-trees/notebook.ipynb)
    - How do we get this model into a machine learning service and serve as API?
- In module 5 we talked about pickling model and loading inside of a Flask app
    - The issue is that depending on an ML framework, there are specific things you have to do in order to save it properly
        - Even within minor versions of the same framework
    - Need to look at documentation and save it the proper way

- BentoML gives you a simple way of saving the model
    - Goes through all of the things you have to do depending on the framework and version and saves it the right way
- Install with `pip install bentoml`
    - Any 1.0+ version is fine

- Editing copy of [Notebook](../notebook.ipynb)
    - Go to end of section 6.9
    - Add:
        ```
        import bentoml
        bentoml.xgboost.save_model("credit_risk_model", model)
        ```
    - Saves locally with the given name and a random version id
        - E.g. `Model(tag="credit_risk_model:roq6vjcubkeqiaav", path="/home/andre/bentoml/models/credit_risk_model/roq6vjcubkeqiaav/")`
            - Tag `roq6vjcubkeqiaav`
- Now we will create the service: [Script](../service.py)
    - Load model reference with `bentoml.xgboost.get()`
    - Create **runner**
        - `model_ref.to_runner()`
        - Abstraction layer for model itself
            - Allows model to scale separately from overall service
        - For now just know it is how we access the model and call `.predict()`
    - Create **service**
        - `bentoml.Service('credit_risk_classifier', runners=[model_runner])
        - Easy to way to package multiple models together if needed
    - Create model endpoint
        - Make function `def classify(application_data):`
        - Predict `prediction = await model_runner.predict.async_run(vector)`
            - Runner has exact same signatures as the model did originally
            - Only difference is that you call `predict.run()` or `predict.async_run()`
    - For now just return "{ "status": "Approved"}" for all calls
    - Add function **decorator**
        - `@svc.api(input=JSON(), output=JSON())`
        - Make sure to import `from bentoml.io import JSON`
- Call service with `bentoml serve service.py:svc`
    - Now have service running locally at http:/0.0.0.0:3000/
    - Comes up with a Swagger UI in browser
        - Swagger is an automatically generated UI based off of the OpenAPI spec
        - OpenAPI is a standard way of describing APIs
        - BentoML automatically generates API spec so Swagger can generate UI for testing API
    - Can see /classify has same signature as we gave it when we created the service
    - Some other health endpoints
    - Look inside inference API in webpage
        - Hit `Try it out` button
        - Use same dummy data from last module:
            ```
            {
                "seniority": 3,
                "home": "owner",
                "time": 36,
                "age": 26,
                "marital": "single",
                "records": "no",
                "job": "freelance",
                "expenses": 35,
                "income": 0.0,
                "assets": 60000.0,
                "debt": 3000.0,
                "amount": 800,
                "price": 1000
            }
            ```
        - We get an error `TypeError: Not supported type for data.<class 'dict'>`
            - Recall we were not sending data right to model. We were doing a dict vectorize before
        - What do we do if we don't have this vectorizer in our service?
            -  BentoML gives us easy way of saving objects with model
        - Add `custom_objects={"object_name": object}` to `.save_model()`:
            ```
            bentoml.xgboost.save_model(
                'credit_risk_model',
                model,
                custom_objects={
                    'dictVectorizer': dv
            })
            ```
    - Can use code in [Notebook](../code/train.ipynb)
    - Got tag "credit_risk_model:vrwnb4subwmjaaav"
        - Replace in service and get custom objects
            - `dv = model_ref.custom_objects['dictVectorizer']`
            - Need EXACT same spelling as when saved
            - Vectorize input to function then predict on vector
        - May not always want to use tag every time. Can use **latest**
            - `model_ref = bentoml.xgboost.get(""credit_risk_model:latest"")`
    - Restart service
        - Can **restart service automatically** by adding option `--reload`
            - `bentoml serve service.py:svc --reload`
    - Try to test again
    - Works!

- Add logic to do different things based on prediction
    - Add some decision boundaries based on `prediction[0]`
        ```
        if result > 0.5:
            return {
                "status": "DECLINED"
            }
        elif result > 0.25:
            return {
                "status": "MAYBE"
            }
        else:
            return {
                "status": "APPROVED"
            }
        ```
    - Can run basically any code in this function to make ML workflow as flexible as possible
- Test again
- Works!