# Overview

[KServe](https://github.com/kserve/kserve) provides a Kubernetes Custom Resource Definition for serving machine learning (ML) models on arbitrary frameworks. 
- It aims to solve production model serving use cases by providing performant, high abstraction interfaces
- Supports common ML frameworks like Tensorflow, XGBoost, ScikitLearn, PyTorch, and ONNX.

Scroll down to [ðŸ›« Create your first InferenceService](https://kserve.github.io/website/master/get_started/first_isvc/)
- Show's YAML for an InferenceService
    ```
    kubectl apply -n kserve-test -f - <<EOF
    apiVersion: "serving.kserve.io/v1beta1"
    kind: "InferenceService"
    metadata:
    name: "sklearn-iris"
    spec:
    predictor:
        model:
        modelFormat:
            name: sklearn
        storageUri: "gs://kfserving-examples/models/sklearn/1.0/model"
    EOF
    ```
    - Much less YAML than normally needed

KServe was a part of KubeFlow - was a FOSS ML platform than runs on Kubernetes
- Thing on Kube that allows a lot of things, namely serving models
- Things that could be in Kubeflow
    - Notebooks
    - Pipelines
    - Many languages
    - GPU management
- KServe used to be KubeServing/KFServing
    - Only just became KServe

We will only follow KServe now that it is independent.

## Recap

In Kubernetes section we deployed model with a TF-Serving container and gateway
- Fashion website example, used CNN to identify type of clothes and suggest to use
- Separate gateway service preprocessed image from website and sent to TF-Serving service
    - Converted input to protobuf and sent over gRPC
    - Post-processed result and sent to user web page
- Deployed this w/ Kubernetes
    - 4 YAML files:
        - Deployment + Service for TF-Serving
        - Deployment + Service for Gateway
- With KServe this is one YAML

## KServer
Can define a pre/post processing service and model(s) with one YAML
    - 1 YAML
    - Describes everything (model, preprocessing, postprocessing)
    - Instead of making Docker images, we keep model on S3, TF-Serving fetches model and loads it
        - KServe uses a transformer on another pod to handle everything but the model
        - All handled from one YAML but generates two pods/services and can use gRPC (defaults to HTTP)

So we want to convert the same model to KServe from Kubernetes
- Before that, want to examine simple use case: Scikit-learn model
    - The KServe custom Kube object is called InferenceService, or ISVC
        - The 1 YAML we described was also 1 ISVC
- We will deploy the churn prediction model to ISVC
    - User data/features will be sent as JSON
    - User churn probability will be returned as user approved/not approved

KServe has **Scikit-learn server** - pod
    - Also Pytorch server, PMML server, XGB server, LGB server
    - E.g. using our tree model from prior sections via XGB server
        - KServe figures it out automatically
    - Model will live in S3 bucket (or some accessible storage)
    - No pre/post processing here so user request will right to Sklearn server and back

KServe has general ModelServer class so you can use a general one or provide your own
- Basic API primitives to easily build custom model serving 
    - Can build custom model serving image or use something like BentoML
- Has the following serverless features:
    - Scale to and from Zero
    - Request based Autoscaling on CPU/GPU
    - Revision Management
    - Optimized Container
    - Batching
    - Request/Response logging
    - Traffic management
    - Security with AuthN/AuthZ
    - Distributed Tracing
    - Out-of-the-box metrics
    - Ingress/Egress control

## Overview

1. Introduction (this)
2. Running KServe locally
    * Installing KServe locally with kind
    * Deploying an example model from documentation
3. Deploying an sklearn model with KServe
    * Trainign the churn model with specific sklearn version
    * Deploying the churn prediction model with KServe
4. Deploying custom sklearn images with KServe
    * Customizing the sklearn image
    * Running KServe service locally
5. Serving TF models with KServe
    * Converting the Keras model to saved_model format
    * Deploying the model
    * Preparing the input
6. Kserve transformers
    * Why do we need transformers?
    * Creating a service for pre/post processing
    * Using existing transformers
7. Deploying with KServe and EKS
    * Creating an EKS cluster
    * Installing KServe on EKS
    * Setting up S3 access
    * Deploying the clothing model
8. Summary
    * Less YAML, faster deployment
    * Less stability
    * The need for Ops is not gone