Transcription of [[course-zoomcamp/03-classification/notes/notebook#3 6 Feature importance Mutual Information]]
# 3.6 Feature importance: Mutual information

Mutual information - concept from information theory, it tells us how much 
we can learn about one variable if we know the value of another

* https://en.wikipedia.org/wiki/Mutual_information

We are using this to measure the importance of a categorical variable.

Previously: we looked at risk ratio to see the importance of categorical variables. Applies to each value within a variables.
- E.g. variable `contract`
    - Can see that people on month-to-month contracts are more likely to churn than those on plans.

We can see that `contract` is important but not if it is more or less important than others. 

Intuition here: the higher the mutual information is, the more we learn about `churn` from a variable.

```python
from sklearn.metrics import mutual_info_score
```

```python
mutual_info_score?
```

    [0;31mSignature:[0m [0mmutual_info_score[0m[0;34m([0m[0mlabels_true[0m[0;34m,[0m [0mlabels_pred[0m[0;34m,[0m [0;34m*[0m[0;34m,[0m [0mcontingency[0m[0;34m=[0m[0;32mNone[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
    [0;31mDocstring:[0m
    Mutual Information between two clusterings.
    
    The Mutual Information is a measure of the similarity between two labels
    of the same data. Where :math:`|U_i|` is the number of the samples
    in cluster :math:`U_i` and :math:`|V_j|` is the number of the
    samples in cluster :math:`V_j`, the Mutual Information
    between clusterings :math:`U` and :math:`V` is given as:
    
    .. math::
    
        MI(U,V)=\sum_{i=1}^{|U|} \sum_{j=1}^{|V|} \frac{|U_i\cap V_j|}{N}
        \log\frac{N|U_i \cap V_j|}{|U_i||V_j|}
    
    This metric is independent of the absolute values of the labels:
    a permutation of the class or cluster label values won't change the
    score value in any way.
    
    This metric is furthermore symmetric: switching :math:`U` (i.e
    ``label_true``) with :math:`V` (i.e. ``label_pred``) will return the
    same score value. This can be useful to measure the agreement of two
    independent label assignments strategies on the same dataset when the
    real ground truth is not known.
    
    Read more in the :ref:`User Guide <mutual_info_score>`.
    
    Parameters
    ----------
    labels_true : int array, shape = [n_samples]
        A clustering of the data into disjoint subsets, called :math:`U` in
        the above formula.
    
    labels_pred : int array-like of shape (n_samples,)
        A clustering of the data into disjoint subsets, called :math:`V` in
        the above formula.
    
    contingency : {ndarray, sparse matrix} of shape             (n_classes_true, n_classes_pred), default=None
        A contingency matrix given by the :func:`contingency_matrix` function.
        If value is ``None``, it will be computed, otherwise the given value is
        used, with ``labels_true`` and ``labels_pred`` ignored.
    
    Returns
    -------
    mi : float
       Mutual information, a non-negative value, measured in nats using the
       natural logarithm.
    
    Notes
    -----
    The logarithm used is the natural logarithm (base-e).
    
    See Also
    --------
    adjusted_mutual_info_score : Adjusted against chance Mutual Information.
    normalized_mutual_info_score : Normalized Mutual Information.
    [0;31mFile:[0m      ~/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages/sklearn/metrics/cluster/_supervised.py
    [0;31mType:[0m      function


Order does not matter.

```python
mutual_info_score(df_full_train.churn, df_full_train.contract)
```




    0.0983203874041556



Significant mutual information. If we know the contract type, we do learn a lot about potential churn.

```python
mutual_info_score(df_full_train.gender, df_full_train.churn)
```




    0.0001174846211139946



Very low mutual information. If we know the gender, we do not learn much about potential churn.

```python
mutual_info_score(df_full_train.partner, df_full_train.churn)
```




    0.009967689095399745



Noticable mutual information. Not as much as contract type but much more than gender.

These are numbers are hard to interpet on their own, but we can tell the differences.

What we can do, is check the mutual information of every variable and order them.

```python
def mutual_info_churn_score(series):
    return mutual_info_score(series, df_full_train.churn)
```

```python
# .apply() variable allows us to run a function on a Pandas Series
mi = df_full_train[categorical].apply(mutual_info_churn_score)  # only applies to categorical variables
mi.sort_values(ascending=False)
```




    contract            0.098320
    onlinesecurity      0.063085
    techsupport         0.061032
    internetservice     0.055868
    onlinebackup        0.046923
    deviceprotection    0.043453
    paymentmethod       0.043210
    streamingtv         0.031853
    streamingmovies     0.031581
    paperlessbilling    0.017589
    dependents          0.012346
    partner             0.009968
    seniorcitizen       0.009410
    multiplelines       0.000857
    phoneservice        0.000229
    gender              0.000117
    dtype: float64



- `contract` very important
- `onlinesecurity` to `dependents` are decently important.
- Notice a drop in order of magnitude at `partner`, and another for `multiplelines`
- We thought `partner` was relatively important, but was not important in the larger scheme of things

These useful variables are why ML actually works. Variables like contract, onlinesecurity, and techsupport actually give information on churn. 

These are the signals ML models use while training and allow them to make inference on unseen examples.

# Summary

**Mutual information**: concept from information theory. Measures how much we can learn about one variable if we know the value of another. 

We use `mutual_info_score(x, y)` from Scikit-Learn to calculate the mutual information between the *x* feature variable and *y* target variable. 